# Monitoring urban growth and the urban heat island effect using Earth observation

## Introduction

Chapter 1 outlined the concepts and issues of poorly planned urban development and the Urban Heat Island (UHI) effect. Chapter 2 builds upon this base knowledge through discussing current monitoring approaches and associated limitations that are addressed within this thesis.

## Monitoring land cover change

Land cover data can be collected using traditional field surveys, however, for expansive urban metropolitan areas these are often unfeasible due to the required funding, person hours, strategic planning and annual replication for complete temporal and spatial coverage. As a result, data derived from field surveys is often incomplete, spatially aggregated, and temporally and geographically limited. Comparatively, satellite remote sensing enables efficient extraction of geographic land cover changes with considerable cost reductions in a timely and synoptic manner (Powell et al., 2007; Yuan et al., 2005). This is achieved through classification of unique surface reflectance signatures per pixel, being the fraction of incoming solar radiation reflected by the Earth’s surface over a defined area (e.g. 30 m2) into predefined land cover classes. 

Classifying an Earth Observation (EO) image in determining land cover change can be summarised by three main steps: preprocessing, classification and output evaluation. The majority of recent research advancements focus on classification, with comparatively fewer and more established methods for preprocessing and output evaluation. Three main image analysis approaches exist for extracting land cover estimates from EO data: classification algorithms, spectral indices and data-fusion approaches. Classification algorithms assign one or more user-defined land cover classes to a pixel in the digital image. For example, GlobeLand30 provided global land cover estimates divided into 10 classes, namely water bodies, wetland, artificial surfaces, cultivated land, snow/ice, forest, shrubland, grassland, bare earth and tundra from Landsat data with an overall accuracy exceeding 80% (Chen et al., 2015). However, the difficulties of extracting built-up areas is a known issue within the remote sensing community, resultant from spectral similarities between natural surface materials such as bare soil and man-made materials such as impervious surfaces leading to spectral confusion during classification (Herold et al., 2002; Lu et al., 2011; Varshney and Rajesh, 2014). New per-pixel spectral indices, such as the Normalized Difference Built-up Index (NDBI) (Zha et al., 2003), provide an alternative approach in determining the presence of a sole land cover class (e.g. urban), with a user-determined threshold establishing the value at which a pixel is assigned to the land cover type (Angiuli and Trianni, 2013; Xu, 2008; Zha et al., 2003). However, Schneider (2012) suggested analysis must move beyond mere consideration of spectral information to the temporal, spatial or polarimetric domain in order to resolve misclassification, particularly in an urban environment. In this sense, additional variables are obtained or computed and appended to original imagery for classification algorithm accuracy improvement. In the following sections an overview of preprocessing and output evaluation is provided, with a comprehensive literature review of current classification methodologies. 

### Image preprocessing

Image preprocessing entails correction for noise unattributed to surface reflectance, such as radiometric (atmospheric), geometric (image projection) and topographic (physical features) errors (Hansen and Loveland, 2012). Satellite data results in images experiencing differing radiometric conditions defined as the sensitivity of the sensor to incoming reflectance, due to variations in atmospheric conditions, solar illumination, sensor calibration, view angle and soil and vegetation changes (Du et al., 2002; Yang and Lo, 2000). Similarly, geometric misalignment and slope orientation in relation to incoming solar radiation can result in inconsistences when undertaking scene classification and thematic evaluation (Richter et al., 2009). The majority of studies exploring land use and land cover change have implemented medium spatial resolution (30 m) Landsat imagery, obtaining the longest, free temporal image repository of consistent medium spatial resolution data, with a temporal resolution of 16 days (Bagan and Yamagata, 2012a; Kressler and Steinnocher, 2001; Lu et al., 2011; Sexton et al., 2013; Sundarakumar et al., 2012; Tan et al., 2009). 
Landsat data are distributed as a surface reflectance product achieved through the Landsat Ecosystem Disturbance Adaptive Processing System (LEDPAS) and the Landsat 8 Surface Reflectance algorithm (L8SR) otherwise known as the Landsat 8 Surface Reflectance Code (LaSRC) for correction of atmospheric conditions (Hansen and Loveland, 2012; USGS, 2015). The former corrects for atmospheric effects using the Second Simulation of a Satellite Signal in the Solar Spectrum (6S) radiative transfer model, whilst the latter implements an internally developed algorithm (Hansen and Loveland, 2012; USGS, 2015). Additional processing to Level 1 Terrain-corrected data (L1T) corrects for both geometric and topographic errors using ground control points and a Digital Elevation Model (DEM) from the Global Land Survey (GLS) 2000 data set (Hansen and Loveland 2012). However, owing to the requirement of several parameters (i.e. Aerosol Optical Thickness (AOT), ozone and air temperature) for surface reflectance derivation assumptions or models of values are often implemented (Ju et al., 2012). Consequently, for removal of remaining post-atmospheric correction noise such as the brightening effect of cloud or darkening of cloud shadow, Sexton et al. (2013) put forward the notion of image standardisation based on pre-defined band specific values and subsequent normalisation for reduced inter-annual surface reflectance values. When classifying EO imagery over multiple years this approach permits the use of a single classification model as opposed to individual classification for each time point considered. 

### Image classification

Holistic image classification is achieved through a supervised or unsupervised methodology. In a supervised classification the user selects sample pixels termed Regions of Interest (ROI) that are representative of predefined land cover classes. ROIs are then input to a classification algorithm that identifies pixels with similar reflectance values to each provided land cover class for entire image classification. During an unsupervised classification an algorithm automatically separates image pixels into clusters obtaining similar spectral characteristics, with only a user defined number of classes. Owing to the majority of recent literature focusing on supervised classifiers due to the complexity and size of datasets this review shall be limited to supervised methodologies (Schneider, 2012). 

### Recent classification methodologies

Classifier selection is dependent on the nature of input data and desired output, defined as parametric or nonparametric. Parametric algorithms make assumptions regarding ROIs selected for training, such as a Gaussian distribution, whereas nonparametric algorithms do not make this assumption (Donnay and Unwin, 2001; Jensen, 2005). Popular traditional nonparametric classification algorithms include density slicing, parallepiped, minimum distance to mean, and nearest-neighbour, with Maximum Likelihood (ML) the most widely utilised parametric classifier (Jensen, 2005). 

#### Maximum likelihood 

The ML classification is based on Bayes’ Theorem of decision making. It assigns each pixel to the most probable user-defined land cover class, rather than the minimum distance, through considering both the variances and covariances of class signatures (Atkinson and Lewis, 2000; Jensen, 2005). The ML algorithm permits specification of prior classification probability information (i.e. expected frequency of classes per scene). However, in reality, information of this sort is rarely available, with the majority of ML applications assuming equal class probability per scene and assigning land cover classification to pixels based upon the highest probability. The advantage of the ML classifier pertains to assignment based on probability often used in determining land cover (Fuller et al., 1994). 

#### Spectral mixture analysis

More recently, parametric Spectral Mixture Analysis (SMA) and subsequently machine learning or ‘expert systems’ have been implemented to solve classification problems (Jensen, 2005; Okujeni et al., 2014). SMA considers the selection of spectrally unique endmembers, with image data being assigned the most appropriate match (Powell et al., 2007). It is based on the assumption that reflectance measured at each pixel is represented by the linear sum of endmembers weighted by the associated endmember fraction. In standard SMA a set number of representative endmembers, commonly between two and five, are extracted, with the entire image being modelled on their spectral characteristics (Powell et al., 2007). Endmember extraction normally revolves around identification of spectral extremes (e.g. Adams, 1995). However, selection of a limited number of extreme endmembers results in an inability to adequately represent the high spectral heterogeneity of the urban landscape (Powell et al., 2007). Consequently endmembers may not fully represent image spectral variability or a pixel may be modelled by endmembers that do not represent materials within its field of view. Both factors result in a reduction of classification accuracy (Powell et al., 2007). Due to being an original approach that could consider multiple endmembers whilst improving accuracy of ML and minimum distance approaches SMA has been used in establishing Vegetation-Impervious surface-Soil (V-I-S) fractions (Phinn et al., 2002) and analysing impervious surface distributions (Wu and Murray, 2003). 

#### Multiple endmember spectral mixture analysis

Multiple Endmember Spectral Mixture Analysis (MESMA) extends this methodology through permitting the number and type of endmembers to alter on a per pixel basis attempting to represent the inherent spectral variability within land cover types over the entire image as a linear combination of constituent components (Okujeni et al., 2015, 2013; Powell et al., 2007; Weng and Pu, 2013). Mixture models are iteratively calculated for each pixel, comparing all possible endmember formulations, deriving the fit between measured and modelled signals. The model obtaining the lowest Root Mean Square Error (RMSE) is designated to the pixel (Okujeni et al., 2013). For each land cover class the MESMA library should obtain sufficient spectra to competently represent spectral variability. However, as the overall number of endmembers increases computational efficiency exponentially decreases. Thus, the endmember library should remain adequately small to maximise computational efficiency, whilst obtaining land cover spectral diversity within selected spectra (Powell et al., 2007). Due to the advantages of multiple endmember selection MESMA has been implemented in classifying land cover (Franke et al., 2009) and mapping forest fire burn severity levels (Quintano et al., 2013).  

#### Machine learning algorithms 

In contrast, Machine Learning Algorithms (MLAs) use an automated inductive approach for identification of patterns in data (Cracknell and Reading, 2014). The majority of research focusing on MLAs surrounds the predication of land cover from multi-spectral or hyperspectral surface reflectance measurements (Angiuli and Trianni, 2013; Braun et al., 2012; Rodriguez-Galiano et al., 2012; Schneider, 2012). MLA classification is derived by a discrimination function y=f(x), with inputs expressed as vectors d of the form (x_1,x_2,…,x_d), where y  is a definitive set of c class labels (y_1,y_2,…,y_c). Using instances of x and y supervised machine learning trains the classification model, mapping image data to defined classes (Cracknell and Reading, 2014). Popular MLAs include Artificial Neural Networks (ANNs), Random Forests (RFs) and Support Vector Machines (SVMs). 

#### Artificial neural network 

Nonparametric ANNs are an interconnected group of nodes that use mathematical methods to process information in a self-adaptive system, attempting to ‘mimic’ a human brain (Bhatta, 2010; Hu and Weng, 2009). The Multi-Layer Perceptron (MLP) feed forward network is the most popular ANN; obtaining three layers - one input, one hidden and one output layer - each comprising of several nodes (artificial neurons). The input layer represents the original image, with each band representing one node. Classification is undertaken in the hidden layer, with results presented in the output layer. The learning ability originates from the learning algorithm, with the most popular being Back-Propagation (BP) otherwise known as delta rules (Hu and Weng, 2009). Learning is achieved through node weight assignment, with training samples input into the model. If the difference between the produced results and test sample is larger than the initial threshold, weights are altered for difference minimisation. The process is iterated until a pre-defined accuracy level is obtained or maximum iterations reached and a classified land cover output is produced (Candade and Dixon, 2004; Cracknell and Reading, 2014; Hu and Weng, 2009). ANNs have been widely used due to their robustness and ability to learn complex patterns, successfully implemented in ship detection (Tang et al., 2015), tree detection (Malek et al., 2014) and land use classification (Cheng and Han, 2016; Hu and Weng, 2009; Pacifici et al., 2009).

#### Random forest 

RFs are a nonparametric ensemble learning method that implement a majority vote system to predict classes based on data partition from multiple Decision Trees (DT) (Breiman, 2001; Cracknell and Reading, 2014). Multiple trees are created, using a random subset of input features to reduce generalisation error, with the end user specifying the number of trees to be developed and number of features at each node. Each tree implements a bagging sample permitting growth based on differing training subsets, with a search across a random selection of input variables for derivation of a split per node (Cracknell and Reading, 2014; Gislason et al., 2006; Rodriguez-Galiano et al., 2012). Bagging facilitates training data creation through randomly resampling the original dataset, with each selected subset for tree growth containing a proportion of the training dataset. Samples not selected are input to the Out Of Bag subset (OOB). OOB samples not utilised for tree training can be classified by the tree for performance evaluation (Rodriguez-Galiano et al., 2012). DT design requires the determination of an attribute section and pruning method. The random forest classifier implements the Gini Index as the attribute selector method, measuring the impurity of an attribute compared to classes (Pal, 2005). DT can be constrained through a termination criterion threshold limiting growth size and therefore overfitting, termed pre-pruning. Additionally post-pruning techniques permit overall performance evaluation, due to being pruned with validation data. However, Breiman (1999) suggested that whilst the number of trees increases the generalisation error always converges without overfitting due to the Strong Law of Large Numbers, which states that the average results obtained from a large number of trials (or trees) should be near the expected value and will become closer with the more trials performed (Rodriguez-Galiano et al., 2012). Thus, for classification, each tree within the RF inputs a vote for the most popular class, with the output classification determined by the majority of tree votes (Gislason et al., 2006). RF are advantageous over other ensemble classification methodologies such as boosting and bagging through an improved methodological process and less intensive computational requirement being used in instances to classify: land cover (Gislason et al., 2006) and tree species (Immitzer et al., 2012).

#### Support vector machine 

The nonparametric SVM classifier identifies an optimal maximum margin separating hyperplane, dividing the dataset into the predefined number of classes, with points on the margins termed support vectors (Braun et al., 2012; Foody and Mathur, 2006, 2004; Mountrakis et al., 2011; Qian et al., 2014; Vapnik and Chervonenkis, 1971). The underlying benefit of SVM is known as structural risk minimisation, whereby SVMs are able to minimise error on unseen data without prior assumptions made on the data probability distribution (Mountrakis et al., 2011). SVMs are linear binary classifiers assigning participant pixels into one of two possibilities. However, remote sensing derived land covers are often not linearly separable due to cluster overlap. Consequently implementation of soft margin and kernel methods aid inseparability through transforming data into high dimensional feature spaces (Euclidean or Hilbert) utilising non-linear functions to identify linear solutions (Braun et al., 2012; Mountrakis et al., 2011). In order to prevent over fitting SVM implements a two-dimensional grid search using stratified crossvalidation to search for the kernel (g) and regularisation parameter (C); (g) defines the width of the Gaussian kernel function whilst (C) controls training data and decision boundary maximisation plus margin errors (Zhu and Hastie, 2005). For derivation of more than two land cover classes additional methodological processes are required, common methods include one-against-all, one-against-one and directed acyclic graph SVM, whereby the binary nature of SVM is iterated in differing formants to derive the appropriate land cover classification (Chih-Wei et al., 2008; Mountrakis et al., 2011). SVMs are one of the most prominent and effective MLA due to structural risk minimisation applied to a variety of applications including land cover change detection (De Morsier et al., 2013), airport detection (Tao et al., 2011) and road extraction (Cheng and Han, 2016; Das et al., 2011).

### Comparison of recent classification methodologies

Image classification accuracy is dependent on the selected classification methodology and the choice of internal parameters (Huang et al., 2002; Watanachaturaporn et al., 2008). Due to the parametric nature of the ML classifier it can often fail to represent land cover that might be multimodal, thus in certain circumstances ML has been outperformed by alternative classification algorithms (Melgani and Bruzzone, 2004; Mountrakis et al., 2011; Otukei and Blaschke, 2010; Watanachaturaporn et al., 2008). Similarly MESMA classification can be inefficient owing to additional computational demands associated with an increasing numbers of endmembers which often precludes selection and is consequently considered a more traditional method when compared to MLAs  (Okujeni et al., 2015; Ram and Wang, 2013). In a comparison of ML, DT (e.g. RF), ANN and SVM classifiers, Watanachaturaporn et al. (2008) and Kotsiantis et al. (2006) found the SVM classifier to produce optimal accuracy. Similarly Huang et al. (2002) found SVMs obtain a higher accuracy than ML, DTs and ANNs indicating that the superior performance of SVM is attributed to the derivation of an optimal separating hyperplane (Foody and Mathur, 2006, 2004; Huang et al., 2002; Mountrakis et al., 2011). Whilst no single MLA can uniformly outperform all other MLAs across all data sets, in terms of overall accuracy the majority of literature preferences implementation of SVM due to its self-adaptability, efficient learning speed and limited training data requirements (Kotsiantis et al., 2006; Mountrakis et al., 2011).

### Image spectral combinations

The reliable and accurate identification and extraction of built-up areas from medium resolution EO imagery (e.g. Landsat) is a known issue within the remote sensing community; originating from spectral heterogeneity of urban surfaces often resulting in spectral confusion during image classification (Herold et al., 2002; Lu et al., 2011; Varshney and Rajesh, 2014). Consequently new spectral indices, which in the most part do not require classification have been postulated as an alternative and more computational efficient approach.

Zha et al. (2003) proposed a built up index termed the Normalized Difference Built-up Index (NDBI) algorithm for identification of built up regions using the reflective bands: Red, Near-Infrared (NIR) and Mid-Infrared (MIR). NDBI makes the assumption that built up area has a high spectral reflectance in the MIR compared to the NIR. However, MIR vegetated spectral response can increase above NIR under drier conditions (Gao, 1996; Xu, 2008). Thus, Zha et al. (2003) implemented the Normalised Difference Vegetation Index (NDVI) to filter noise arising from vegetation. Nevertheless Xu (2008) stated that sole use of original spectral bands for construction of a built-up land index is inappropriate due to the composition of complex spectral features. 

Consequently Xu (2008) followed the methodological framework of Ridd (1995), that the spatial composition of urban areas can be decomposed into Vegetation-Impervious surface-Soil creating the V-I-S model with the inclusion of water, grouping the urban area into: built up land, vegetation and open water (Ridd, 1995). Three indices of NDBI, the Soil Adjusted Vegetation Index (SAVI) and Modified Normalized Difference Water Index (MNDWI) represented the land cover categories respectively. MNDWI modifies the Normalized Difference Water Index (NDWI) through selection of the MIR band in place of the NIR band, remediating built up land noise for open water selection. SAVI was preferenced over NDVI due to greater sensitivity in detecting vegetation in low-plant covered regions such as urban areas, estimated to work with plant cover as low as 15% compared to NDVI at 30% (Xu, 2008). Aforementioned indices extracting unique features were then combined in the Index-based Built-up Index (IBI). However, the intrinsic issue of IBI pertains to selection of an appropriate user defined correction value for SAVI ranging from 0 for high plant densities to 1 for low plant densities. Furthermore complete land cover is assumed to be adequately modelled from built land, vegetation and water. Thus, analysis has resulted in urban area remaining mixed with bare earth, requiring additional polygon layers defining the urban region from an unspecified source to filter erroneous built up land for definitive extraction (Stathakis et al., 2012; Sun et al., 2015; Xu, 2008; Zha et al., 2003). Additionally, the nature of determining appropriate singular threshold values over heterogeneous urbans fails in global practicality and has the potential for the introduction of localised errors impacting reliability (Xu, 2008).

In contrast to the threshold approach presented by Xu (2008), Angiuli and Trianni (2013) proposed the Normalised Difference Spectral Vector (NDSV). Due to multiple indices presented throughout literature NDSV attempts a simultaneous merge for the production of intrinsically normalised globally consistent data whilst reducing ambiguities associated within individual indexes (Angiuli and Trianni, 2013; Patel et al., 2015). NDSV computes all possible indices through the combination of all bands. Thus, with 6 Landsat bands a total of 30 indexes are generated, but due to the symmetry of definition, 15 are negative representations of other indexes (Angiuli and Trianni, 2013; Patel et al., 2015). NDSV creates a normalised signature per pixel and is subsequently classified. Nevertheless, any index is founded upon assumptions, for example, NDVI is based on the rationale that green plants absorb solar radiation in the photosynthetically active radiation spectral region (400 – 700 nm) and reflect radiation in the NIR region. Therefore NDVI is usually highly correlated with Leaf Area Index (LAI) and has found to be sensitive to canopy background variations such as soil visible through the canopy (Jensen, 2009). Consequently NDVI can be unsatisfactory, especially when mapping senesced vegetation owing to reduced absorption in the visible bands and reflection in the NIR band (Jensen, 2005). This methodology also fails to directly extract urban extents owing to the requirement of a classification model. 

### Data fusion methodologies

Recent data fusion methodologies combine additional or computed data to existing spectral bands in order to improve classification accuracy (Rodriguez-Galiano et al., 2012). Popular approaches can be categorised into spatial, temporal and polarimetric domains extracting additional information from texture, temporal composites and radar respectively. The following sections outline and compare these procedures, determining current research trends and establishing the most appropriate methodological approach in quantifying the temporal urban growth section of the overall thesis aim.

#### Spatial domain

Texture analysis provides a representation of the visual characteristics of an image permitting incorporation of spatial information into image (e.g. Landsat) classification found to produce more accurate classifications of heterogeneous land covers such as urban (Møller-Jensen et al., 2005; Rodriguez-Galiano et al., 2012; Zhou and Troy, 2008). Co-occurrence texture measures such as mean, variance, homogeneity, contrast, dissimilarity, entropy, second moment and correlation are computed using a moving rectangular window surrounding a central pixel. Nevertheless, an overarching issue pertains to window size; it must be large enough to capture variance, yet small enough to represent homogenous land cover (Møller-Jensen et al., 2005). Consequently, the window-based approach tends to smooth boundaries between discrete land cover types determined from medium-coarse resolution imagery, with the appropriate window size being difficult to discern and a rectangular window not necessarily representative of real land coverage (Møller-Jensen et al., 2005). Very high resolution (< 1 m) surface reflectance imagery, Light Detection and Ranging (LiDAR) and stereo imagery such as that procured during Perth’s Urban Monitoring project and the State of Indiana’s strategic plan can overcome these limitations, but are associated with high financial outlay and often infrequent repeat collections that currently preclude extensive temporal monitoring (e.g. 15 years) (Caccetta et al., 2012; The State of Indiana, 2017). 

#### Temporal domain

Annual and multi-seasonal temporal image composites increase class spectral separability through stacking imagery from multiple dates into a single image (Bhatta, 2010; Castrence et al., 2014; Schneider, 2012; Sexton et al., 2013; Yuan et al., 2005; Zhu et al., 2012). This follows the logic that natural surfaces obtain a type of cyclical pattern resulting from changes in the proportion of land cover (e.g. mixtures of vegetation, soil and water) based on the time of year (Jensen, 2005). However, when natural surfaces are replaced with impervious structures the fluctuation will cease owing to the conversion to built-up land cover generally being unidirectional, identifiable from a multi-temporal signature in spectral space (Castrence et al., 2014). Regardless, the premise of this method is founded upon the assumption that limited or no change will have occurred within a complete temporal period of stacked imagery. Ideally, each variable used in classification should enable additional refinement for improved accuracy. Nevertheless, due to the number of bands within the multi-temporal image composite high variable correlation may be prevalent (Bhatta, 2010; Zhu et al., 2012). Redundancy can be overcome through principal component transformation, with components containing significant variance selected for classification (Bhatta, 2010). Whilst Zhu et al. (2012) acknowledged this issue through investigating the effect of increasing variables during classification they concluded that although some variables contribute relatively little, the trend is straightforward; more independent data results in higher classification accuracy. 

#### Polarimetric domain 

Synthetic Aperture Radar (SAR) data are playing an increasingly important role in remote sensing owing to all weather operational ability (Zhu et al., 2012). Although SAR images over urban areas provide low quality images due to problems associated with radar imaging in such an environment (i.e. multiple bouncing, layover and shadowing), SAR texture measures can provide valuable information in discerning urban areas (Dell’Acqua et al., 2003; Zhu et al., 2012). Isolated scattering of residential areas and crowded backscatters of inner city high density areas permit classification refinement, thus textural measures such as those descried within the spatial domain can aid identification of alternative urban forms (Zhu et al., 2012). However, the lack of freely available SAR data that temporally coincides with other satellite imagery (e.g. Landsat) frequently precludes extensive use.

### Output evaluation 

Accuracy assessment of classified data is key to ensure effective and appropriate data usage. Accuracy can be determined through visual inspection, non-site specific analysis, difference imaging, error budgeting and quantitative assessments (Congalton, 2001). Visual inspection is often the first step of assessment in ensuring the production of a valid output, but does not provide numerical quantification. Non-site specific analysis and difference imaging compare classified output between an alternative data source for a small spatial area and complete image respectively, providing a spatial component to map error. However, these methods fail in determining the accuracy of each individual land cover class, presented as difference in area estimates and difference images. Error budgeting estimates the total error of a project workflow based on analyst attributed values, combined in an error index (Congalton, 2001). Whilst this assists in determining and assessing data input, user and methodological error potential it fails in end user classification output accuracy estimation. A quantitative accuracy assessment is imperative in order to accurately report any modelled urban growth estimates, often omitted from values provided in metropolitan planning documents. An error matrix is the most common quantitative evaluation of classified remotely sensed data (Foody and Mathur, 2004; Friedl et al., 2010; Van de Voorde et al., 2011; Watanachaturaporn et al., 2008). An error matrix is a square array comparing the number of sample units correctly determined by the classifier in relation to a data source (e.g. original image or Google Earth) per land cover category. Outputs include (i) user’s accuracy defined as the fraction of correctly classified pixels relative to all others classified as a particular land cover, (ii) producer’s accuracy defined as the fraction of correctly classified pixels compared to ground truth data, and (iii) overall accuracy that represents the combined fraction of correctly classified pixels across all land cover types (Congalton, 2001). Quantitative accuracy metrics of this sort permit appropriate use of land cover products and parameterisation of further analysis that expands upon the classified output such as recent Urban Heat Island (UHI) studies that combine land cover data and satellite derived temperature. 

### Classification methodological conclusion

Mapping urban areas remains a complex challenge owing to the complex variation of materials and geometries that compose the urban environment and contribute to mixed spectral signatures (Schneider, 2012). Methodologies employed for extraction of urban areas from satellite imagery are diverse and often location dependent, with no current standardised best practice for urban monitoring. Throughout the literature spectral, spatial, temporal and polarimetric data have been used in differing formulations for urban area extraction. 

Due to the limited past record of complete SAR data, required temporal analysis observed within academic and metropolitan studies (e.g. 15 years) is often unfeasible. Additionally, spectral analysis can be seasonally dependent, whilst spatial analysis can remove underlying trends through data smoothing and poor representation of real land cover due to a definitive rectangular moving window. Generation of unique multi-temporal spectral signatures increases the amount of independent data available for classification but the underlying assumption of minimal change between composited images is made. Due to these limitations, EO data is typically classified as standalone data (e.g. surface reflectance) by classification algorithms discussed in section 2.2.3. However, these methodologies have been found to significantly over or underestimate urban area by between 50-60% in complex landscapes such as the urban-rural frontier (Lu et al., 2011; Wu and Murray, 2003). Improving our ability to map urban area is currently an essential challenge due to the potential for classified land cover products to inform decision making such as determining future development strategies and informing further environmental analysis and policies (Bagan and Yamagata, 2014; Hepinstall-Cymerman et al., 2013; Miller and Small, 2003; Schneider et al., 2005). 

## Monitoring urban heat islands

Two forms of temperature affecting urban areas are frequently monitored in relation to an expanding urban area; air temperature and Land Surface Temperature (LST). The former often pertains to traditional meterological monitoring, whereas the latter is based on thermal measurements made from EO data. The following sections describe these two methodological approaches.

### Traditional urban heat island methodologies

Air temperature represents Urban Canopy Layer (UCL) temperature; directly impacting human comfort and public health, monitored from static weather stations (Guo et al., 2014). For example, Shanghai’s heatwave excess mortality rates (Tan et al., 2010), Hong Kong’s UHI-mortality association (Goggins et al., 2012) and Melbourne’s UHI economic assessment (AECOM Australia, 2012) used differenced temperature data from meteorological stations in rural and urban geographical locations to determine the UHI Intensity (UHII) (Tan et al., 2010). To account for the spatial variation in the UHI effect, the temperature measurements from weather stations are often spatially interpolated. The accuracy of the interpolated dataset can be dependent on the type of interpolation method, the number of points available and distance between them (Hattis et al., 2012). Consequently, whilst studies using point-based meteorological data provide a broad city scale view of the UHI, they are impractical for targeted mitigative planning actions (e.g. urban greening) due to the limited number of meteorological stations in many areas. 

### Remotely sensed land surface temperature 

EO data overcomes the limitations of point based methods through providing near global coverage of LST on a per pixel basis using instruments that measure in thermal spectral wavebands. LST measurements are widely used to quantify the impact of land cover type on the Surface Heat Island (SHI), often related to air temperature in the same location, resulting in the term Surface Urban Heat Island (SUHI) (Schwarz et al., 2012; Voogt and Oke, 2003). In the context of characterising the UHI, LST is typically used, as opposed to air temperature due to the additional parameters required to compute air temperature such as surface properties, atmospheric conditions and solar angles that must be incorporated, assuming data availability during satellite overpass.
Due to their advantages in monitoring temperature, satellite instruments including the Moderate Resolution Imaging Spectroradiometer (MODIS) (Wang et al., 2015), Landsat Thematic Mapper (TM) (Jimenez-Munoz et al., 2014; Rinner and Hussain, 2011; Sobrino et al., 2004), the geostationary Spinning Enhanced Visible and Infrared Imager (SEVIRI) (Blasi et al., 2016) and the Advanced Along-Track Scanning Radiometer (AATSR) (Fabrizi et al., 2010) have been used to monitor LST and the UHI effect. Nevertheless, current methodologies frequently fail in planning practicality due to the static temporal nature through consideration of limited (two or less) EO temperature images (Li et al., 2011; Tomlinson et al., 2011) alongside aggregation to broad land cover types or use of singular metrics such as the UHII (Cao et al., 2010; Imhoff et al., 2010; Zhou et al., 2016). For example Li et al. (2011) used temperature extracted from two Landsat images captured in March and July 2001 to infer the effects of landscape composition and configuration on the UHI in Shanghai. Whilst their results produced strong correlations between LST and landscape metrics, selection of single images obtained during spring (March) and summer (July) disregard the temporal component of LST (e.g. the complete annual temperature cycle) and fail to account for potential abnormalities in temperature on selected days (e.g. heatwaves) (Li et al., 2011). 

Similarly, whilst Zhou et al. (2016) explored the spatio-temporal trends of the UHI throughout China using daily MODIS LST between 2003 and 2016, their analysis was restricted to comparison of the UHII using land cover data from only 2005 and 2010. The use of two classified land cover images restricted UHI analysis through the assumption of unchanged urban area between 2003-2007 (for the 2005 image) and 2008-2012 (for the 2010 image). Land cover changes within these timeframes had the potential to produce erroneous results alongside sole output of the UHII that precludes quantification of changes in land cover associated with temperature for targeted policy remediation (Zhou et al., 2016). Consequently, research must adapt to consider the needs and requirements of metropolitan development frameworks in order to assist in more sustainable future metropolitan development.

### Localised temperature mitigation

In response to the UHI effect and updated international policies outlined in section 1.5 a variety of localised mitigation measures have ensued, categorised into voluntary and policy themes. The former represents demonstrative projects and incentives such as Sacramento’s Tree Foundation providing free shade trees to Sacramento residents (USA Environmental Protection Agency, 2013). The latter incorporates the UHI into metropolitan frameworks such as Perth and Peel @3.5million (Western Australian Planning Commission, 2015a), The London Plan (Mayor of London, 2016a) and Johannesburg’s Spatial Development Framework 2040 (City of Johannesburg Metropolitan Municipality, 2016). **However, these policies frequently fail in planning practicality through lacking any specific methodological requirement**. Consequently local governments have incorporated quantifiable policy requirements such as Seattle’s Green Factor specifying minimum vegetation requirements, yet lacking placement guidelines that could result in sub-optimal locations (USA Environmental Protection Agency, 2013). Other local governments such as the City of Perth and Fremantle have initiated EO informed Urban Forest programmes to maintain and increase vegetation coverage (City of Fremantle, 2017; City of Perth, 2006). However, due to the lack of scientifically applied UHI mitigation studies and devolution of targets to local governments, varied, inconsistent and aggregated block scale LST methodologies provide the potential to misinform vegetation placement (City of Fremantle, 2017; City of Perth, 2006). The majority of academic literature implementing remotely sensed data analysing the UHI effect uses medium-low resolution satellite imagery (e.g. MODIS, 1 km and Landsat, 30 m), inappropriate for very small scale, localised UHI mitigation decisions. It is therefore imperative to provide policy-relatable methodologies in order to facilitate scientifically-valid decision making in ensuring the sustainability of our cities.
